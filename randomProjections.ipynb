{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import hamming as hammingDist\n",
    "import pandas as pd\n",
    "%run basicVSM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProjection:\n",
    "      \n",
    "    #setup code to project the collection\n",
    "    def __init__(self,collection: pd.DataFrame,inverted_index,idf,doHash = False, numberOfRandomVectors = 10, distribution = 'achlioptas'):\n",
    "                             \n",
    "\n",
    "        '''sets up everything in order to retrieve with random projections. Attention, takes some time to execute.\n",
    "\n",
    "        collection           please provide data frame with preprocessed documents\n",
    "        inverted_index       please provide the inverted index\n",
    "        idf                  please provide the IDF scores\n",
    "        doHash               whether to perform a hashing as presented in the lecture, or not which will maybe be a little slower, but way more accurate and is the normal way in the literature I found\n",
    "        numberOfRandomVectors\n",
    "        distribution         random vectors are generated from one of the three different possibilities:\n",
    "                             'achlioptas': a distribution considered most efficient in the literature, see e.g. https://cs.gmu.edu/~jessica/publications/lsi_sdm_workshop03.pdf\n",
    "                             'normal': a normal distribution\n",
    "                             'randomSamples': chooses random documents as random vectors\n",
    "\n",
    "        there is no explicit load and safe anymore as the class can be simply pickled'''\n",
    "       #split collection in id column and collection as list\n",
    "        self.collection =list(collection['text'])\n",
    "        self.docIDs = list(collection['id'])\n",
    "\n",
    "        self.inverted_index=inverted_index\n",
    "        self.idf = idf\n",
    "\n",
    "        print('create random matrix with ',distribution,' distribution')\n",
    "        self.randomMatrix = self.createRandomMatrix(distribution,numberOfRandomVectors)\n",
    "\n",
    "        if(doHash):\n",
    "            print('project the collection with ',numberOfRandomVectors, \"random vectors and hashing\")\n",
    "        else:\n",
    "            print('project the collection with ',numberOfRandomVectors, \"random vectors without hashing\")\n",
    "        self.docMatrix, self.threshold = self.projectCollection(numberOfRandomVectors, doHash)\n",
    "\n",
    "\n",
    "\n",
    "    #create matrix of random vectors \n",
    "    def createRandomMatrix(self,distribution,numberOfRandVecs):\n",
    "        #(Achlioptas random distribution is more efficient according to: https://cs.gmu.edu/~jessica/publications/lsi_sdm_workshop03.pdf)\n",
    "        NumberOfTerms = len(self.inverted_index)\n",
    "        if(distribution=='achlioptas'):\n",
    "            #sparse version not suitable as matrix is too sparse\n",
    "            #values = (np.sqrt(3),0,-1*np.sqrt(3))\n",
    "            #weights = (1/6,2/3,1/6)\n",
    "            #R = np.random.choice(a = values,p = weights, size= numberOfRandVecs*NumberOfTerms)\n",
    "            #dense version\n",
    "            values = (1,-1)\n",
    "            R = np.random.choice(a = values, size= numberOfRandVecs*NumberOfTerms)\n",
    "            R = np.reshape(R,(numberOfRandVecs,NumberOfTerms))\n",
    "            normalize(R,axis = 1,copy = False)\n",
    "        #a normal distribution\n",
    "        elif(distribution == 'normal'):\n",
    "            R = np.random.random((numberOfRandVecs,NumberOfTerms))\n",
    "            normalize(R,axis = 1,copy = False)\n",
    "        #get a sample from the document collection\n",
    "        elif(distribution == 'randomSamples'):\n",
    "            docs = np.random.randint(len(self.collection), size = numberOfRandVecs)\n",
    "            R = np.zeros((numberOfRandVecs,NumberOfTerms))\n",
    "            for i in range(numberOfRandVecs):\n",
    "                R[i]+= createVector(self.collection[docs[i]],self.inverted_index,self.idf)\n",
    "            normalize(R,axis = 1,copy = False)\n",
    "        else:\n",
    "            raise ValueError(\"The distribution param was not specified correctly. It accepts 'achlioptas','normal''randomSamples'\")\n",
    "        return R\n",
    "\n",
    "    #random projects without threshold\n",
    "    def projectCollection(self, numberOfRandVecs, doHash = False):\n",
    "        NumberOfDocs = len(self.collection)\n",
    "        rtn = np.zeros((NumberOfDocs,numberOfRandVecs))\n",
    "        #for every document\n",
    "        for i in range(NumberOfDocs):\n",
    "            if (i%100 == 0):\n",
    "                print(i) # to see that it didn't crash\n",
    "            d = createVector(self.collection[i],self.inverted_index,self.idf) # create vector for document\n",
    "            d=normalize(d.reshape(1,-1))[0] # such that \n",
    "            #calc dot product with all random vectors\n",
    "            inner = d@self.randomMatrix.T\n",
    "            # inset into result matrix\n",
    "            rtn[i] += inner\n",
    "        #hash against the median of a each column of the vector to achieve maximum entropy. \n",
    "        #Will output the threshold to use for query hashing\n",
    "        if (doHash == True):\n",
    "            threshold = np.median(rtn, axis=0)\n",
    "            rtn = rtn>threshold\n",
    "        else:\n",
    "            threshold = None\n",
    "        return (rtn,threshold) \n",
    "\n",
    "    def projectQuery(self, query):\n",
    "        d = createQueryVector(query,self.inverted_index)\n",
    "        d=normalize(d.reshape(1,-1))[0] \n",
    "        #calc dot product with all random vectors\n",
    "        inner = d@self.randomMatrix.T\n",
    "        #hash \n",
    "        if self.threshold is not None:\n",
    "            return inner > self.threshold\n",
    "        else:\n",
    "            inner=normalize(inner.reshape(-1,1)) \n",
    "        return inner\n",
    "\n",
    "    #retrieval on new matrix\n",
    "    def retrieveWithRandom(self, query, amount= 100):\n",
    "        '''\n",
    "        retrieval method\n",
    "\n",
    "        query   as list of unique terms'''\n",
    "        #project the query\n",
    "        if self.threshold is None:\n",
    "            q = self.projectQuery(query)\n",
    "            #compute similarity of all projected data\n",
    "            sim = self.docMatrix.dot(q)\n",
    "        else:\n",
    "            sim = np.zeros(len(self.docIDs))\n",
    "            q = self.projectQuery(query)\n",
    "            #compute Hamming distance\n",
    "            for d in range(len(self.docIDs)):\n",
    "                sim[d] = 1 - hammingDist(self.docMatrix[d],q)\n",
    "        #sort output and cut to first 100\n",
    "        results = {}\n",
    "        for i in range(len(sim)):\n",
    "            results[self.docIDs[i]]=sim[i]\n",
    "        rtn = sorted(results.items(), key=lambda kv: kv[1],reverse = True)\n",
    "        if(len(rtn) > amount):\n",
    "            rtn = rtn[0:amount]\n",
    "        return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#random projects without threshold\n",
    "def projectCollection(randomMatrix, collection, inverted_index, NumberOfRandVecs, doHash = False):\n",
    "    R = randomMatrix\n",
    "    NumberOfDocs = len(collection)\n",
    "    rtn = np.zeros((NumberOfDocs,NumberOfRandVecs))\n",
    "    #for every document\n",
    "    for i in range(NumberOfDocs):\n",
    "        if (i%100 == 0):\n",
    "            print(i) # to see that it didn't crash\n",
    "        d = createVector(i,inverted_index,collection) # create vector for document\n",
    "        d=normalize(d.reshape(1,-1))[0] # such that \n",
    "        #calc dot product with all random vectors\n",
    "        inner = d@randomMatrix.T\n",
    "        # inset into result matrix\n",
    "        rtn[i] += inner\n",
    "    #hash against the median of a each column of the vector to achieve maximum entropy. \n",
    "    #Will output the threshold to use for query hashing\n",
    "    if (doHash == True):\n",
    "        threshold = np.median(rtn, axis=0)\n",
    "        rtn = rtn>threshold\n",
    "    else:\n",
    "        threshold = None\n",
    "    return (rtn,threshold) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def projectQuery(query, randomMatrix, inverted_index,threshold = None):\n",
    "    d = createQueryVector(query,inverted_index)\n",
    "    d=normalize(d.reshape(1,-1))[0] \n",
    "    #calc dot product with all random vectors\n",
    "    inner = d@randomMatrix.T\n",
    "    #hash \n",
    "    if threshold is not None:\n",
    "        return inner > threshold\n",
    "    else:\n",
    "        inner=normalize(inner.reshape(-1,1)) \n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#retrieval on new matrix\n",
    "def retrieveWithRandom(query, docMatrix, randomMatrix, inverted_index,docIDs, amount= 100, threshold = None):\n",
    "    #project the query\n",
    "    if threshold is None:\n",
    "        q = projectQuery(query=query,randomMatrix=randomMatrix,inverted_index = inverted_index)\n",
    "        #compute similarity of all projected data\n",
    "        sim = docMatrix.dot(q)\n",
    "    else:\n",
    "        sim = np.zeros(len(docIDs))\n",
    "        q = projectQuery(query = query,randomMatrix = randomMatrix, inverted_index= inverted_index, threshold = threshold)\n",
    "        #compute Hamming distance\n",
    "        for d in range(len(docIDs)):\n",
    "            sim[d] = 1 - hammingDist(docMatrix[d],q)\n",
    "    #sort output and cut to first 100\n",
    "    results = {}\n",
    "    for i in range(len(sim)):\n",
    "        results[docIDs[i]]=sim[i]\n",
    "    rtn = sorted(results.items(), key=lambda kv: kv[1],reverse = True)\n",
    "    if(len(rtn) > amount):\n",
    "        rtn = rtn[0:amount]\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def retrieveWithRandomWrapper(query, Model, amount = 100):\n",
    "    '''retrieves with random projections given a Model\n",
    "    \n",
    "    query    query as list of unique terms\n",
    "    Model    the model containing all 'class variables' for random projections without using the class structure\n",
    "    amount   how many results to show'''\n",
    "    return retrieveWithRandom(query, Model['docMatrix'], Model['randomMatrix'], Model['inverted_index'],Model['docIDs'],amount, Model['threshold'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#setup code to project the collection\n",
    "def randomProjectionSetup(collection: pd.DataFrame,inverted_index,doHash = False, numberOfRandomVectors = 10, distribution = 'achlioptas',\n",
    "                          load = False, safe = True, path = \"randomProjectionModel.psave\"):\n",
    "    \n",
    "    '''sets up everything in order to retrieve with random projections. Attention, takes some time to execute.\n",
    "    \n",
    "    collection           please provide docs_preprocessed\n",
    "    inverted_index       please provide the inverted index\n",
    "    doHash               whether to perform a hashing as presented in the lecture, or not which will maybe be a little slower, but way more accurate and is the normal way in the literature I found\n",
    "    numberOfRandomVectors\n",
    "    distribution         random vectors are generated from one of the three different possibilities:\n",
    "                         'achlioptas': a distribution considered most efficient in the literature, see e.g. https://cs.gmu.edu/~jessica/publications/lsi_sdm_workshop03.pdf\n",
    "                         'normal': a normal distribution\n",
    "                         'randomSamples': chooses random documents as random vectors\n",
    "    \n",
    "    load  is either false and the setup will be performed, or true and the model will be restored from path\n",
    "    safe  defaults to true and indicates whether the model should be saved to path on creation.\n",
    "    path  contains the standard path to the model saves to and loads from. May be changed.'''\n",
    "    if(load):\n",
    "        with open(path,\"rb\") as f:\n",
    "            Model = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        Model = {}\n",
    "        #split collection in id column and collection as list\n",
    "        Model['collection']=list(collection['text'])\n",
    "        Model['docIDs'] = list(collection['id'])\n",
    "        \n",
    "        Model['inverted_index']=inverted_index\n",
    "        Model['doHash'] = doHash\n",
    "        Model['numberOfRandomVectors'] = numberOfRandomVectors\n",
    "        Model['distribution'] = distribution\n",
    "        \n",
    "        print('create random matrix with ',distribution,' distribution')\n",
    "        Model['randomMatrix'] = createRandomMatrix(numberOfRandomVectors, Model['collection'], inverted_index, distribution)\n",
    "        \n",
    "        if(doHash):\n",
    "            print('project the collection with ',numberOfRandomVectors, \"random vectors and hashing\")\n",
    "        else:\n",
    "            print('project the collection with ',numberOfRandomVectors, \"random vectors without hashing\")\n",
    "        Model['docMatrix'], Model['threshold'] = projectCollection(Model['randomMatrix'], Model['collection'], inverted_index, \n",
    "                                                       numberOfRandomVectors, doHash)\n",
    "\n",
    "        \n",
    "        if(safe):\n",
    "            with open(path,\"wb\") as f:\n",
    "                pickle.dump(Model,f)\n",
    "    return Model\n",
    "#R = createRandomMatrix()\n",
    "#docMatrixHash, t = projectCollection(randomMatrix= R, doHash = True)\n",
    "#docMatrix = projectCollection(randomMatrix=R)\n",
    "#docMatrix = docMatrix[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
