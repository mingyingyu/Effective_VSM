{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "%run utils.ipynb\n",
    "import time as tt\n",
    "dev_df = pd.read_csv('data/dev.df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file sets up all the models which were used for evaluation\n",
    "data structures are loaded by the \"docs2df.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file_path = 'data/nfcorpus/raw/stopwords.large'\n",
    "rare_words_file_path = 'data/rare_tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_ori(dev_df,stop_word_file_path,rare_words_file_path)\n",
    "with open('data/devProcessed.psave',\"wb\") as f:\n",
    "    pickle.dump(dev_df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/devProcessed.psave',\"rb\") as f:\n",
    "            dev_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = createInvertedIndex(dev_df,load=True,path='data/invertedIndexDevOri.psave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get Leaders\n",
      "compute similarities of documents to leaders\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "compute cluster matricies\n",
      "in total there are  56  clusters\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "fitting preclustering:  761.0739769935608\n"
     ]
    }
   ],
   "source": [
    "#fit sparseVSM\n",
    "%run sparseBasicVSM.ipynb\n",
    "start = tt.time()\n",
    "sps = SparseBasicVSM(dev_df,inverted_index,path='data/sparseVSM.psave')\n",
    "end = tt.time()\n",
    "print('fitting sparse VSM: ',end-start)\n",
    "#fit preclustering\n",
    "%run SparsePreclustering.ipynb\n",
    "idf = createIDF(inverted_index,len(dev_df))\n",
    "start = tt.time()\n",
    "spre = SparsePreclustering(dev_df,inverted_index,idf)\n",
    "end = tt.time()\n",
    "print('fitting preclustering: ',end-start)\n",
    "\n",
    "with open('data/preclustering.psave',\"wb\") as f:\n",
    "    pickle.dump(spre,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run randomProjections.ipynb\n",
    "for h in [True,False]:\n",
    "    start = tt.time()\n",
    "    randP = RandomProjection(dev_df,invertedIndex,idf,numberOfRandomVectors=1500,doHash=h)\n",
    "    end= tt.time()\n",
    "    print('time for fitting the model: ',end-start)\n",
    "    with open('/data/randP'+str(1500)+'Hash'+str(h)+'.psave',\"wb\") as f:\n",
    "            pickle.dump(randPnoH,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
