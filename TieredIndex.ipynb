{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "%run preprocessing_ti.ipynb\n",
    "%run basicVSM_TIversion.ipynb\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TierIndexBuilder:\n",
    "    \n",
    "    inverted_index = ''\n",
    "    number_tier = ''\n",
    "    global_tier={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, path_to_docs, path_to_index, path_to_tw_matrix, stop_words_file_path, rare_words_file_path):\n",
    "        self.df_docs = pd.read_csv(path_to_docs)\n",
    "    \n",
    "    def raw_frequency(self, term, doc):\n",
    "        count = 0\n",
    "        if isinstance(doc, str):\n",
    "            for word in doc.split():\n",
    "                if term == word:\n",
    "                    count = count + 1\n",
    "        return count\n",
    "\n",
    "    def get_most_freq_term(self, doc):\n",
    "        doc_freq = dict([word, self.raw_frequency(word, doc)] for word in doc.split())\n",
    "        value, count = collections.Counter(doc_freq).most_common(1)[0]\n",
    "        return count\n",
    "\n",
    "    def compute_tf(self, term, doc):\n",
    "        if (self.raw_frequency(term, doc) > 0):\n",
    "            return (1 + np.log10(self.raw_frequency(term, doc))) / (1 + np.log10(self.get_most_freq_term(doc)))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def build_inverted_index(self, docs_df:pd.DataFrame):\n",
    "        print('building inverted_index...')\n",
    "        inverted_index = collections.defaultdict(dict)\n",
    "        for i in tqdm(range(len(docs_df))):\n",
    "            existing=set()\n",
    "            doc_id = docs_df['id'][i]\n",
    "            doc_text = docs_df['text'][i]\n",
    "            for term in str(doc_text).split():\n",
    "                 if term not in existing:\n",
    "                    tf = self.compute_tf(term, doc_text)\n",
    "                    inverted_index[term][doc_id] = tf\n",
    "                    existing.add(term)\n",
    "        #print(inverted_index)\n",
    "        return self.sort_inverted_index(inverted_index)\n",
    "    \n",
    "    def sort_inverted_index(self, inverted_index):\n",
    "        sorted_inverted_index=collections.defaultdict(dict)\n",
    "        for i in inverted_index:\n",
    "            sorted_inverted_index[i]=dict(sorted(inverted_index[i].items(),key=operator.itemgetter(1), reverse=True))\n",
    "            #print(sorted_inverted_index)\n",
    "        return sorted_inverted_index\n",
    "    \n",
    "    def store_sort_inverted_index(self, inverted_index, inverted_index_path):\n",
    "        pickle.dump(inverted_index, open(inverted_index_path,'wb'))\n",
    "\n",
    "    def build_tiers(self, inverted_index,tier_type, fix_length, threshold):\n",
    "        print('building tiers..')\n",
    "        global_tier=collections.defaultdict(dict)\n",
    "        for term in tqdm(inverted_index):\n",
    "            if tier_type==\"fixedLength\":\n",
    "                local_tier=self.build_fixed_local_tier(list(inverted_index[term].items()),fix_length)\n",
    "            else:\n",
    "                local_tier=self.build_threshold_local_tier(list(inverted_index[term].items()),threshold)\n",
    "            for tier in local_tier:\n",
    "                global_tier[tier][term]=local_tier[tier]\n",
    "                global_tier[tier][term].sort(key=operator.itemgetter(0))\n",
    "        \n",
    "        return global_tier\n",
    "\n",
    "    def build_fixed_local_tier(self,inverted_index_list:list, fix_length):\n",
    "        local_tier=collections.defaultdict(list)\n",
    "        number_tier=(int(len(inverted_index_list)/fix_length)+1)\n",
    "        for tier_id in range(number_tier):\n",
    "            step_tier=tier_id*fix_length\n",
    "            for doc_index in range(step_tier,step_tier+fix_length):\n",
    "                if doc_index==len(inverted_index_list):\n",
    "                     break\n",
    "                local_tier[tier_id].append()\n",
    "        return dict (local_tier)\n",
    "    \n",
    "   \n",
    "    def build_threshold_local_tier(self, inverted_index_list:list, threshold):\n",
    "        local_tier=collections.defaultdict(list)\n",
    "#         print(inverted_index_list)\n",
    "        df_posting=pd.DataFrame(columns=['doc_id','weight'])\n",
    "        for i in inverted_index_list:\n",
    "            df_posting.loc[len(df_posting)]=i\n",
    "        tier=0\n",
    "        while tier<=len(threshold):\n",
    "            tier_docs = list()\n",
    "            if tier == 0:\n",
    "                document=df_posting.loc[df_posting['weight']>threshold[tier]]\n",
    "                tier_docs.extend([tuple(x)for x in document.values ])\n",
    "            elif tier==len(threshold):\n",
    "               \n",
    "                document = df_posting.loc[df_posting['weight'] <=threshold[tier-1]]\n",
    "                tier_docs.extend([tuple(x) for x in document.values])\n",
    "            else:\n",
    "                ttt  = df_posting.loc[df_posting['weight']>threshold[tier]]\n",
    "                document = ttt[ttt['weight']<=threshold[tier-1]]\n",
    "                tier_docs.extend([tuple(x) for x in document.values])\n",
    "            for a in tier_docs :\n",
    "                local_tier[tier].append(a)\n",
    "            tier=tier+1\n",
    "        return dict(local_tier)\n",
    "        \n",
    "    def storeGlobalTier(self,global_tier,global_tier_path):\n",
    "        pickle.dump(global_tier,open (global_tier_path,'wb'))\n",
    "        \n",
    "    def countNumTierTerm(self,term, global_tier):\n",
    "            count=0\n",
    "            for tier in global_tier:\n",
    "                for key , value in dict(global_tier[tier]).items():\n",
    "                    if key==term:\n",
    "                        count=count+1\n",
    "            return count\n",
    "        \n",
    "    def createterm_numberoftier(self,inverted_index,global_tier):\n",
    "            number_tier=collections.defaultdict(int)\n",
    "            for term in inverted_index.keys():\n",
    "                number_tier[term]=self.countNumTierTerm(term,global_tier)\n",
    "            return number_tier\n",
    "\n",
    "    def storeNumbertier(self,number_tier,number_tier_path):\n",
    "        pickle.dump(number_tier,open(number_tier_path,'wb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__=='__main__':\n",
    "#     # run from current jpy __name__='__main__'\n",
    "#     # import by other file :__name__='TieredIndex' , just module name\n",
    "#     path_to_inverted_index=r'../data/inverted_index.p'\n",
    "#     path_to_index=r'../data/index_all_docs.p'\n",
    "#     path_to_tw_matrix=r'../data/matrix.p'\n",
    "#     stop_words_file_path=r'../data/nfcorpus\\raw\\stopwords.large' \n",
    "#     rare_words_file_path=r'../data/rare_tokens.txt'\n",
    "#     path_to_docs=r'../data/dev.df'\n",
    "#     builder=TierIndexBuilder(path_to_docs, path_to_index, path_to_tw_matrix, stop_words_file_path, rare_words_file_path)\n",
    "#     inverted_index = builder.build_inverted_index(pd.read_csv(path_to_docs))\n",
    "#     builder.store_sort_inverted_index(path_to_inverted_index)### inverted index\n",
    "#     global_tier = builder.build_tiers(inverted_index,'Threshold',40,[0.7,0.6,0.2])\n",
    "#     global_tier_path=r'../data/global_tier.p'\n",
    "#     builder.storeGlobalTier(global_tier,global_tier_path)\n",
    "#     number_tier_path =r'../data/number_tier.p'\n",
    "#     number_tier=builder.createterm_numberoftier(inverted_index,global_tier)\n",
    "#     builder.storeNumbertier(number_tier,number_tier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tierIndexSearch:\n",
    "    def __init__(self,stop_words_path, rare_words_path,inverted_index_path,number_tier_path\n",
    "               ,global_tier_path):\n",
    "        self.preprocess=Preprocessor(stop_words_path, rare_words_path)\n",
    "        self.inverted_index=pickle.load(open(inverted_index_path, \"rb\"))\n",
    "        self.global_tier=pickle.load(open(global_tier_path, \"rb\"))\n",
    "        self.number_tiers=pickle.load(open(number_tier_path,'rb'))\n",
    "        self.maxnumberofTier=len(self.global_tier)\n",
    "        \n",
    "        self.doc_vector = None\n",
    "        \n",
    "    def search(self, query, number_documents, inverted_index, doc_processed, idf):\n",
    "        query=self.preprocess.preprocess_line(query)\n",
    "        query_list=list()\n",
    "        \n",
    "        for term in query.split():\n",
    "            if term in self.inverted_index:\n",
    "                query_list.append(term)\n",
    "#         print(query_list)\n",
    "        revelance_doc=list()\n",
    "        if len(query_list)!=0:\n",
    "            if self.maxnumberofTier>50:\n",
    "                revelance_doc=self.searchByFixlength(query_list,number_documents)\n",
    "            else:\n",
    "                revelance_doc=self.searchByThreshold(query_list,number_documents)\n",
    "        sortRevelanceDocs=self.getSortedRevelanceDoc(query, revelance_doc, inverted_index, doc_processed, idf)\n",
    "        return sortRevelanceDocs\n",
    "    \n",
    "    \n",
    "    def searchByFixlength(self,query, number_documents):\n",
    "        revelance_doc=list()\n",
    "        local=self.searchFirstTier(query)\n",
    "        document_merged=self.mergeTierQUery(local)\n",
    "        revelance_doc=document_merged\n",
    "#         local=collections.defaultdict(list)\n",
    "        tier=0\n",
    "        while(len(revelance_doc)<number_documents):\n",
    "            new_query_list=self.pruneexistTiers(query,tier)\n",
    "            if len(new_query_list)!=0:\n",
    "                del revelance_doc[:]\n",
    "                local=self.mergeTierTerm(new_query_list,local,tier)\n",
    "                document_merged=self.mergeTierQUery(local)\n",
    "                revelance_doc=document_merged\n",
    "                tier=tier+1\n",
    "            else:\n",
    "                break\n",
    "        return revelance_doc\n",
    "    \n",
    "    def searchByThreshold(self,query, number_documents):\n",
    "        revelance_doc=list()\n",
    "        new_query_list=list()\n",
    "        for term in query:\n",
    "            if self.checkTermInTier(term,0):\n",
    "                new_query_list.append(term)\n",
    "#         print(new_query_list)\n",
    "        if len(new_query_list)>0:\n",
    "            local=self.searchFirstTier(query)\n",
    "            document_merged=self.mergeTierQUery(local)\n",
    "            revelance_doc=document_merged\n",
    "#             local=collections.defaultdict(list)\n",
    "            tier=0\n",
    "            while(len(revelance_doc)<number_documents):\n",
    "                new_query_list=self.pruneexistTiers(query,tier)\n",
    "                if len(new_query_list)!=0:\n",
    "                    del revelance_doc[:]\n",
    "                    local=self.mergeTierTerm(new_query_list,local,tier)\n",
    "                    document_merged=self.mergeTierQUery(local)\n",
    "                    revelance_doc=document_merged\n",
    "                    tier=tier+1\n",
    "                else:\n",
    "                    if tier<self.maxnumberofTier:\n",
    "                        tier=tier+1\n",
    "                    else:\n",
    "                        break\n",
    "        return revelance_doc\n",
    "    \n",
    "    def save_vector_of_doc(self, inverted_index, doc_processed, idf):\n",
    "        vec = {}\n",
    "        for i,doc in enumerate(tqdm(doc_processed)):\n",
    "            vec[doc[0]] = createVectorTI(i, inverted_index, doc_processed, idf)\n",
    "        pickle.dump(vec, open('/data/doc_vectors.psave','wb'))\n",
    "        \n",
    "    \n",
    "    def getSortedRevelanceDoc(self,query,revelance_doc, inverted_index,doc_processed, idf):\n",
    "        if self.doc_vector is None:\n",
    "            if not os.path.exists('/data/doc_vectors.psave'):\n",
    "                self.save_vector_of_doc(inverted_index, doc_processed, idf)\n",
    "            self.doc_vector = pickle.load(open('/data/doc_vectors.psave','rb'))\n",
    "        \n",
    "        \n",
    "        revelance_doc_id=[x[0] for x in revelance_doc]\n",
    "        \n",
    "#         res_doc = []\n",
    "#         for i in range(len(doc_processed)):\n",
    "#             if doc_processed[i][0] in revelance_doc_id:\n",
    "#                 res_doc.append(i)\n",
    "        \n",
    "        \n",
    "        docVecs = []\n",
    "#         for d in tqdm(res_doc):\n",
    "#             docVecs.append(createVectorTI(d,inverted_index,doc_processed,idf))\n",
    "        for id in revelance_doc_id:\n",
    "            docVecs.append(self.doc_vector[id])\n",
    "        \n",
    "\n",
    "        queryVec = [createQueryVectorTI(query.split(),inverted_index)]\n",
    "        #rank them\n",
    "        results = {}\n",
    "        for i in range(len(docVecs)):\n",
    "            v = [docVecs[i]]\n",
    "            #compute cosine\n",
    "            cos = cosine_similarity(queryVec,v)\n",
    "            #save\n",
    "            results[revelance_doc_id[i]]=cos[0][0]\n",
    "        #sort rtn by similarity\n",
    "        rtn = sorted(results.items(), key=lambda kv: kv[1],reverse=True)\n",
    "        rtn = [x[0] for x in rtn]\n",
    "    \n",
    "        return rtn\n",
    "    \n",
    "    \n",
    "    def searchFirstTier(self,query):\n",
    "        local=collections.defaultdict(list)\n",
    "        for term in query:\n",
    "            if term in self.global_tier[0]:\n",
    "                local[term]=self.global_tier[0][term]\n",
    "        return local\n",
    "\n",
    "        \n",
    "    def mergeUnion(self,a,b):\n",
    "        merge_list=list()\n",
    "        i=0\n",
    "        j=0\n",
    "        doc_b = { x[0]:x[1] for x in b}\n",
    "        doc_a = { x[0]:x[1] for x in a}\n",
    "        while(i<len(a)):\n",
    "            if a[i][0] in doc_b:\n",
    "                weight = a[i][1] + doc_b[a[i][0]]\n",
    "                merge_list.append((a[i][0],weight))\n",
    "            if a[i][0] not in doc_b:\n",
    "                merge_list.append((a[i][0],a[i][1]))\n",
    "            i+=1\n",
    "        while(j<len(b)):\n",
    "            if b[j][0] not in doc_a:\n",
    "                merge_list.append((b[j][0],b[j][1]))\n",
    "            j+=1\n",
    "#             if a[i][0]==b[j][0]:\n",
    "#                 weight= a[i][1]+b[j][1]\n",
    "#                 merge_list.append((a[i][0],weight))\n",
    "#                 i=i+1\n",
    "#                 j=j+1\n",
    "#             else:\n",
    "#                 if a[i][0]<b[j][0]:\n",
    "#                     weight=a[i][1]\n",
    "#                     merge_list.append((a[i][0],weight))\n",
    "#                     i=i+1\n",
    "#                 else:\n",
    "#                     weight=b[j][1]\n",
    "        return merge_list\n",
    "                    \n",
    "                    \n",
    "    def checkTermInTier(self,term,tier):\n",
    "        return True if term in self.global_tier[tier] else False\n",
    "        for key,value in(self.global_tier[tier]).items():\n",
    "            if key==term:\n",
    "                return  True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def pruneQuerySubsequentiers(self, query, tier):\n",
    "        new_query=list()\n",
    "        for term in query:\n",
    "            if self.checkTermInTier(term, tier) and self.checkTermInTier(term, tier+1):\n",
    "                new_query.append(term)\n",
    "        return new_query\n",
    "\n",
    "    def pruneexistTiers(self,query,tier):\n",
    "        new_query=list()\n",
    "        for term in query:\n",
    "            if self.number_tiers[term]>(tier+1):\n",
    "                new_query.append(term)\n",
    "        return new_query\n",
    "\n",
    "\n",
    "    def mergeTierTerm(self, query,local, tier):\n",
    "#         local=collections.defaultdict(list)\n",
    "        if bool(local) is False:\n",
    "            for term in query:\n",
    "                merged_tiers=self.mergeUnion(self.global_tier[tier][term],self.global_tier[tier+1][term])\n",
    "                local[term] = merged_tiers\n",
    "\n",
    "        else:\n",
    "#             local=collections.defaultdict(list)\n",
    "            for term in query:\n",
    "                if term in self.global_tier[tier+1]:\n",
    "                    merged_tiers=self.mergeUnion(local[term],self.global_tier[tier+1][term])\n",
    "                    local[term] = merged_tiers\n",
    "        return local\n",
    "\n",
    "\n",
    "    def mergeTierQUery(self, local):\n",
    "        local=dict(sorted(local.items(),key=lambda item:len(item[1])))\n",
    "        query_terms=list(local.keys())\n",
    "        document_merged=local[query_terms[0]]\n",
    "        for key in range(1,len(query_terms)):\n",
    "            document_merged=self.mergeUnion(document_merged, local[query_terms[key]])\n",
    "        return document_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
