{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import scipy.sparse as sps\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "%run utils.ipynb\n",
    "%run basicVSM.ipynb\n",
    "\n",
    "\n",
    "class SparsePreclustering:\n",
    "    def __init__(self,collection,inverted_index,idf):\n",
    "        '''sets up everything in order to retrieve with preclustering. Attention, takes some time to execute.\n",
    "        As collection, please provide docs_preprocessed. \n",
    "\n",
    "        '''\n",
    "        self.collection=list(collection['text'])\n",
    "        self.docIDs = list(collection['id'])\n",
    "        self.idf = idf\n",
    "        self.inverted_index=inverted_index\n",
    "        print('get Leaders')\n",
    "        self.leader_list, self.leader_matrix = self.getLeaders()\n",
    "        print('compute similarities of documents to leaders')\n",
    "        self.leader_similarity = self.getDocLeaderSimilarity()\n",
    "        print('compute cluster matricies')\n",
    "        self.clusters, self.clusterMatrices = self.createClusters()\n",
    "    \n",
    "    def createNormalisedCSRMatrix(self,collection,inverted_index):\n",
    "        '''\n",
    "        procedure that takes collection of docs and outputs the corresponding CSR Matrix row normalised\n",
    "        '''\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for i in range(len(collection)):\n",
    "     #       print(i)\n",
    "            #look up, bag of words of d\n",
    "            doc = collection[i]\n",
    "     #       print(doc)\n",
    "            #compute term frequency\n",
    "            f = np.unique(doc,return_counts=True)\n",
    "            max_f = np.max(f[1])\n",
    "            tf = 1+np.log(f[1])\n",
    "            tf = tf/(1 + np.log(max_f))\n",
    "            #create vector\n",
    "            for t in range(len(f[0])): # for loop with ints running through the bag of words of doc\n",
    "                term = f[0][t] # the term at position t\n",
    "                #find correct column of term\n",
    "                pos = list(inverted_index.keys()).index(term)\n",
    "                #insert tf-idf of term into sparse matrix baseline structures\n",
    "                data.append(self.idf[term] * tf[t])\n",
    "                rows.append(i)\n",
    "                cols.append(pos)\n",
    "        matrixCOO = sps.coo_matrix((data, (rows, cols)), shape=(len(collection), len(inverted_index.keys())))\n",
    "        matrixCSR = matrixCOO.tocsr()\n",
    "        normalize(matrixCSR,copy = False)\n",
    "        return matrixCSR\n",
    "\n",
    "    def getLeaders(self):\n",
    "        '''\n",
    "        procedure that randomly selects sqrt(len(collection)) leaders and outputs the indices of those leaders\n",
    "        and a sparse matrix containing their document vectors\n",
    "        '''\n",
    "        l=int(math.sqrt(len(self.collection)))\n",
    "        leader_list= random.sample(self.collection,l)\n",
    "        #print(leader_list)\n",
    "        leader_matrix = self.createNormalisedCSRMatrix(leader_list,self.inverted_index)\n",
    "        return (leader_list,leader_matrix)\n",
    "\n",
    "    #function of cosine ranking---query and leader comparison\n",
    "    def leader_retrieveCosine(self,query):\n",
    "        '''\n",
    "        deprecated\n",
    "        '''\n",
    "        #get all relevant docs with boolean retrieval\n",
    "        boolResult = index\n",
    "        #get vectors for all docs\n",
    "        docVecs = [createVector(self.collection[d],self.inverted_index,self.idf) for d in boolResult]\n",
    "        #get query vector\n",
    "        queryVec = [createQueryVector(query,inverted_index)]\n",
    "        #rank them\n",
    "        results = {}\n",
    "        for i in range(len(docVecs)):\n",
    "            v = [docVecs[i]]\n",
    "            #compute cosine\n",
    "            cos = cosine_similarity(queryVec,v)\n",
    "            #save\n",
    "            results[boolResult[i]]=cos[0][0]\n",
    "        #sort rtn by similarity\n",
    "        rtn = sorted(results.items(), key=lambda kv: kv[1],reverse=True)\n",
    "        return rtn\n",
    "\n",
    "    def getDocLeaderSimilarity(self):\n",
    "        '''\n",
    "        compares each document to the leaders (by computing cosine similarity) \n",
    "        and assigns it to the cluster by providing an output which can be used as a mapping\n",
    "        '''\n",
    "        leader_similarity=[]\n",
    "        for d in range(len(self.collection)):\n",
    "            queryVec = createVector(self.collection[d],self.inverted_index,self.idf).reshape(-1, 1)\n",
    "            normalize(queryVec,axis=0,copy= False)\n",
    "            #compute similarity of document to leaders\n",
    "            leaderSimilarities = self.leader_matrix.dot(queryVec)\n",
    "            #get corresponding row\n",
    "            leader = np.argmax(leaderSimilarities)\n",
    "            #result=leader_retrieveCosine(document) \n",
    "            leader_similarity.append(leader)\n",
    "            if((d % 100) == 0):\n",
    "                print(d)\n",
    "        return leader_similarity\n",
    "\n",
    "    def clusterDocs(self,leader_similarities,leader_list): \n",
    "        '''\n",
    "        cluster the close documents to leader\n",
    "\n",
    "        '''\n",
    "        #initialise dic with empty lists and leader positions as keys\n",
    "        leader_cluster={key: [] for key in np.arange(len(leader_list))}\n",
    "        for i in range(len(leader_similarities)):\n",
    "            #insert the index of the doc (i) at the position of its leader (leader_similarity[i])\n",
    "            leader_cluster[leader_similarities[i]].append(i)\n",
    "        return leader_cluster\n",
    "\n",
    "    def createClusters(self):\n",
    "        '''\n",
    "        creates a sparse matrix for every cluster containing all the vectors of the corresponding documents.\n",
    "        This matrices are returned in a list. On top, the clusters are also returned as a list\n",
    "        '''\n",
    "        clusters = self.clusterDocs(self.leader_similarity,self.leader_list)\n",
    "        result = []\n",
    "        print('in total there are ',len(clusters),' clusters')\n",
    "        counter=0\n",
    "        for leader in clusters.keys(): # for every leader\n",
    "            counter+=1\n",
    "            #get the list of docs assigned to leader and subset collection with those\n",
    "            clusterAsCollection = [self.collection[i] for i in clusters[leader]]\n",
    "            #at the implicit index of leader, insert the matrix of its docs\n",
    "            result.append(self.createNormalisedCSRMatrix(clusterAsCollection,self.inverted_index))\n",
    "            if((counter % 10) == 0):\n",
    "                print(counter)\n",
    "        return (clusters,result)\n",
    "\n",
    "    def retrieveWithPreclustering(self,query,amount=100):\n",
    "        '''\n",
    "        query   list of unique terms\n",
    "        amount   '''\n",
    "        #get query vector\n",
    "        queryVec = createQueryVector(query,self.inverted_index).reshape(-1, 1)\n",
    "        normalize(queryVec,axis=0,copy= False)\n",
    "        #compute similarity of query to leaders\n",
    "        leaderSimilarities = self.leader_matrix.dot(queryVec)\n",
    "        #get corresponding matrix\n",
    "        leader = np.argmax(leaderSimilarities)\n",
    "\n",
    "        #retrieve on this matrix\n",
    "        sim = self.clusterMatrices[leader].dot(queryVec)\n",
    "        #translate indices back by looking up in the leader_cluster\n",
    "        #look up the cluster in which the results where found\n",
    "        cluster = self.clusters[leader]\n",
    "        results = {}\n",
    "        for i in range(len(sim)):\n",
    "            results[self.docIDs[cluster[i]]]=sim[i]\n",
    "\n",
    "        #if there are not enough results in the first cluster\n",
    "        if(len(results)<amount):\n",
    "            #things get slow and messy. Code is copied in order to normally not execute it and get better speed.\n",
    "\n",
    "            #sort leader similarities and index them\n",
    "            leaderSimilarities=[-leaderSimilarities[a][0] for a in range(len(leaderSimilarities))]\n",
    "            leadersSorted= np.argsort(leaderSimilarities)\n",
    "            counter = 1\n",
    "            while((len(results)<amount) & (counter < len(leadersSorted))):\n",
    "                #pick the next leader\n",
    "                leader = leadersSorted[counter]\n",
    "                #retrieve on this matrix\n",
    "                sim = self.clusterMatrices[leader].dot(queryVec)\n",
    "                #translate indices back by looking up in the leader_cluster\n",
    "                #look up the cluster in which the results where found\n",
    "                cluster = self.clusters[leader]\n",
    "                for i in range(len(sim)):\n",
    "                    results[self.docIDs[cluster[i]]]=sim[i]\n",
    "                counter +=1\n",
    "\n",
    "        rtn = sorted(results.items(), key=lambda kv: kv[1],reverse = True)\n",
    "        #output only first elements\n",
    "        if(len(rtn) > amount):\n",
    "            rtn = rtn[0:amount]\n",
    "        return rtn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
