{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class Preprocessor:\n",
    "    stop_words = []\n",
    "    ps = PorterStemmer()\n",
    "    rare_words = []\n",
    "\n",
    "    def __init__(self, stop_words_file_path, rare_words_file_path):\n",
    "        self.stop_words = set(line.strip() for line in open(stop_words_file_path,'rb'))\n",
    "        self.load_rare_words(rare_words_file_path)\n",
    "                \n",
    "    def load_rare_words(self, file):    \n",
    "        with open(file, 'rb') as f :\n",
    "            for w in f:\n",
    "                self.rare_words.append(w.strip())\n",
    "\n",
    "    def get_unique_tokens(self, file):\n",
    "        input_file = open(file, \"r\", encoding=\"utf8\")\n",
    "        allWords = list()\n",
    "        vocab_tokens = list()\n",
    "\n",
    "        for line in input_file:\n",
    "            line.rstrip()\n",
    "            words = line.split()\n",
    "            allWords.extend(words)\n",
    "\n",
    "        for word in allWords:\n",
    "            word = re.sub(r'\\b[^\\W\\d_]+\\b', '', word)\n",
    "            word = word.lower().strip()\n",
    "            if not word.isdigit():\n",
    "                if word not in vocab_tokens:\n",
    "                    vocab_tokens.append(word)\n",
    "\n",
    "        return vocab_tokens\n",
    "    \n",
    "    def get_unique_tokens_preprocessed(self, file):\n",
    "        unique = set()\n",
    "        input_file = open(file, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "        for line in input_file:\n",
    "            line.rstrip()\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                unique.add(word)\n",
    "        return unique\n",
    "            \n",
    "    def get_preprocessed_docs(self, file, keyword, keep_rare_words = True, with_stemming= True):\n",
    "        data = self.parse_docs(file.readlines(), keyword, keep_rare_words, with_stemming)\n",
    "        return data\n",
    "\n",
    "    def parse_docs(self, lines, keyword, keep_rare_words, with_stemming= True):\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            docid, line = line.split('\\t', 1)\n",
    "            url, text = line.split('\\t', 1)\n",
    "\n",
    "            if keyword in docid:\n",
    "                docid = docid.replace('\\t', '')\n",
    "                line = self.preprocess_line(text, keep_rare_words, with_stemming)\n",
    "                data.append((docid, line))\n",
    "        return data\n",
    "\n",
    "    def parse_queries(self, lines):\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            docid, text = line.split('\\t', 1)\n",
    "            url, text_whole = text.split('\\t', 1)\n",
    "            title, rest = text_whole.split('\\t', 1)\n",
    "            title = self.preprocess_line(title)\n",
    "            data.append((docid, title))\n",
    "        return data\n",
    "\n",
    "    def preprocess_line(self, line, keep_rare_words = True, with_stemming= True):\n",
    "        # print(line)\n",
    "        line = self.remove_punctuation(line)\n",
    "        words = line.split()\n",
    "        words_to_keep = []\n",
    "        for word in words:\n",
    "            word = self.clean_url(word)\n",
    "            word = word.lower().strip()\n",
    "            if word not in self.stop_words:\n",
    "                if not self.has_more_digits(word):\n",
    "                    if with_stemming:\n",
    "                        word = self.ps.stem(word)\n",
    "                    if keep_rare_words:\n",
    "                        words_to_keep.append(word)\n",
    "                    else:\n",
    "                        if word not in self.rare_words:\n",
    "                            words_to_keep.append(word)\n",
    "\n",
    "            new_line = ' '.join(words_to_keep)\n",
    "\n",
    "        return self.clean_digits(new_line)\n",
    "\n",
    "    def clean_digits(self, line):\n",
    "        words_to_keep = []\n",
    "        words = line.split()\n",
    "        # if\n",
    "        for word in words:\n",
    "            count_digits = 0\n",
    "            count_chars = 0\n",
    "            for w in word:\n",
    "                if w.isdigit():\n",
    "                    count_digits = count_digits + 1\n",
    "                elif w == '-':\n",
    "                    count_digits = count_digits + 1\n",
    "                elif w == '/':\n",
    "                    count_digits = count_digits + 1\n",
    "                else:\n",
    "                    count_chars = count_chars + 1\n",
    "            if count_chars > count_digits:\n",
    "                words_to_keep.append(word)\n",
    "        return ' '.join(words_to_keep)\n",
    "\n",
    "    def clean_url(self, text):\n",
    "        text = re.sub(r'http://www.ncbi.nlm.nih.gov/pubmed/?', '', text)\n",
    "        text = re.sub(r'http://nutritionfacts.org/topics/', '', text)\n",
    "        text = re.sub(r'http://www.ncbi.nlm.nih.gov/pmc/articles/', '', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(self, txt):\n",
    "        txt = txt.replace('-', ' ')\n",
    "        txt = txt.replace('/', ' ')\n",
    "        return re.sub('[^A-Za-z0-9\\s]+', '', txt)\n",
    "\n",
    "    def has_more_digits(self, txt):\n",
    "        count = 0\n",
    "        for ch in txt:\n",
    "            if ch.isdigit():\n",
    "                count = count + 1\n",
    "        if count > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_file_path=r'C:\\Users\\hechen\\Desktop\\nfcorpus\\raw\\stopwords.large' \n",
    "# rare_words_file_path=r'C:\\Users\\hechen\\Desktop\\rare_tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
